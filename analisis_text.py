# -*- coding: utf-8 -*-
"""Analisis_text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bIGaP9e2gUSdBFsHH1ULzQwXbjmCmJIK
"""

## DESCARGAMOS LOS TWEETS EN FORMATO CSV POR MEDIO DEL SIGUIENTE LINK
## https://www.vicinitas.io/free-tools/download-user-tweets?tracker=santipenap
## https://www.vicinitas.io/free-tools/download-user-tweets?tracker=efrainalegre
## ..

# Tratamiento de datos
# ==============================================================================
import numpy as np
import pandas as pd
import string
import re

# Gráficos
# ==============================================================================
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns
#style.use('ggplot') or plt.style.use('ggplot')

# Preprocesado y modelado
# ==============================================================================
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Configuración warnings
# ==============================================================================
import warnings
warnings.filterwarnings('ignore')

# Obtenemos los datos mediante un csv descargado
tweets_santip = pd.read_csv(r"SantiPenap_user_tweets_f.csv", delimiter=';', engine='python',encoding = "ISO-8859-1")
tweets_efrainaleg = pd.read_csv(r"EfrainAlegre_user_tweets_f.csv", delimiter=';', engine='python',encoding = "ISO-8859-1")
tweets_payocub = pd.read_csv(r"ParaguayoCubas_user_tweets_f.csv", delimiter=';', engine='python',encoding = "ISO-8859-1")
tweets_eucliacev = pd.read_csv(r"euclides2023_user_tweets_f.csv", delimiter=';', engine='python',encoding = "ISO-8859-1")

# Miramos las cantidades de tweets a analizar por candidatos
print('Número de tweets de Santi Peña: ' + str(tweets_santip.shape[0]))
print('Número de tweets de Efrain Alegre: ' + str(tweets_efrainaleg.shape[0]))
print('Número de tweets de Payo Cubas: ' + str(tweets_payocub.shape[0]))
print('Número de tweets de Euclides Acevedo: ' + str(tweets_eucliacev.shape[0]))

# Se unen los dos dataframes en uno solo
tweets = pd.concat([tweets_santip, tweets_efrainaleg, tweets_payocub, tweets_eucliacev], ignore_index=True)

# Miramos el dataframe
tweets.head(3)

# Se seleccionan y renombran las columnas de interés
tweets = tweets[['Name', 'Created At', 'Tweet Id', 'Text','Retweets','Favorites','Media Type']]
tweets.columns = ['autor', 'fecha', 'id', 'texto','retweets','favs','media_type']

tweets

# Parseo de fechas
tweets['fecha'] = pd.to_datetime(tweets['fecha'])
tweets.head(3)

"""**Distribución temporal de los tweets**


Dado que cada usuario puede haber iniciado su actividad en Twitter en diferente momento, es interesante explorar si los tweets recuperados solapan en el tiempo.
"""

# Distribución temporal de los tweets
# ==============================================================================
fig, ax = plt.subplots(figsize=(9,4))

for autor in tweets.autor.unique():
    df_temp = tweets[tweets['autor'] == autor].copy()
    df_temp['fecha'] = pd.to_datetime(df_temp['fecha'].dt.strftime('%Y-%m'))
    df_temp = df_temp.groupby(df_temp['fecha']).size()
    df_temp.plot(label=autor, ax=ax)

ax.set_title('Número de tweets publicados por mes')
ax.legend();

"""**Limpieza y Tokenización**


El proceso de limpieza de texto, dentro del ámbito de text mining, consiste en eliminar del texto todo aquello que no aporte información sobre su temática, estructura o contenido. No existe una única forma de hacerlo, depende en gran medida de la finalidad del análisis y de la fuente de la que proceda el texto. Por ejemplo, en las redes sociales, los usuarios pueden escribir de la forma que quieran, lo que suele resultar en un uso elevado de abreviaturas y signos de puntuación. En este ejercicio, se procede a eliminar: patrones no informativos (urls de páginas web), signos de puntuación, etiquetas HTML, caracteres sueltos y números.

Tokenizar un texto consiste en dividir el texto en las unidades que lo conforman, entendiendo por unidad el elemento más sencillo con significado propio para el análisis en cuestión, en este caso, las palabras.
"""

def limpiar_tokenizar(texto):
    '''
    Esta función limpia y tokeniza el texto en palabras individuales.
    El orden en el que se va limpiando el texto no es arbitrario.
    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)
    y re.escape(string.punctuation)
    '''
    
    # Se convierte todo el texto a minúsculas
    nuevo_texto = texto.lower()
    # Eliminación de páginas web (palabras que empiezan por "http")
    nuevo_texto = re.sub('http\S+', ' ', nuevo_texto)
    # Eliminación de signos de puntuación
    regex = '[\\!\\"\\#\\$\\%\\&\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~\\ð\\µ\\¾\\\\»]'
    nuevo_texto = re.sub(regex , ' ', nuevo_texto)
    # Eliminación de números
    nuevo_texto = re.sub("\d+", ' ', nuevo_texto)
    # Eliminación de espacios en blanco múltiples
    nuevo_texto = re.sub("\\s+", ' ', nuevo_texto)
    # Tokenización por palabras individuales
    nuevo_texto = nuevo_texto.split(sep = ' ')   
    # Eliminación de tokens con una longitud < 2
    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]

    
    return(nuevo_texto)

tweets['texto_tokenizado'] = tweets['texto'].apply(lambda x: limpiar_tokenizar(x))
tweets[['texto', 'texto_tokenizado']].head()

tweets

"""**Análisis exploratorio**


A la hora de entender que caracteriza la escritura de cada autor, es interesante estudiar qué palabras emplea, con qué frecuencia, así como el significado de las mismas.

En Python, una de las estructuras que más facilita el análisis exploratorio es el DataFrame de Pandas, que es la estructura en la que se encuentra almacenada ahora la información de los tweets. Sin embargo, al realizar la tokenización, ha habido un cambio importante. Antes de dividir el texto, los elementos de estudio eran los tweets, y cada uno se encontraba en una fila, cumplimento así la condición de tidy data: una observación, una fila. Al realizar la tokenización, el elemento de estudio ha pasado a ser cada token (palabra), incumpliendo así la condición de tidy data. Para volver de nuevo a la estructura ideal se tiene que expandir cada lista de tokens, duplicando el valor de las otras columnas tantas veces como sea necesario. A este proceso se le conoce como expansión o unnest.

Aunque puede parecer un proceso poco eficiente (el número de filas aumenta mucho), este simple cambio facilita actividades de tipo: agrupación, contaje, gráficos..
"""

# Totales por cada autor
# ==============================================================================
print('--------------------------')
print('RTs totales por autor')
print('--------------------------')
tweets.groupby(by='autor')['retweets'].sum()

# Totales por cada autor
# ==============================================================================
print('--------------------------')
print('Favs totales por autor')
print('--------------------------')
tweets.groupby(by='autor')['favs'].sum()

# Unnest de la columna texto_tokenizado
# ==============================================================================
tweets_tidy = tweets.explode(column='texto_tokenizado')
tweets_tidy = tweets_tidy.drop(columns='texto')
tweets_tidy = tweets_tidy.rename(columns={'texto_tokenizado':'token'})
tweets_tidy.head(3)

# Palabras totales utilizadas por cada autor
# ==============================================================================
print('--------------------------')
print('Palabras totales por autor')
print('--------------------------')
tweets_tidy.groupby(by='autor')['token'].count()

# Palabras distintas utilizadas por cada autor
# ==============================================================================
print('----------------------------')
print('Palabras distintas por autor')
print('----------------------------')
tweets_tidy.groupby(by='autor')['token'].nunique()

# Longitud media y desviación de los tweets de cada autor
# ==============================================================================
temp_df = pd.DataFrame(tweets_tidy.groupby(by = ["autor", "id"])["token"].count())
temp_df.reset_index().groupby("autor")["token"].agg(['mean', 'std'])

# Top 5 palabras más utilizadas por cada autor
# ==============================================================================
tweets_tidy.groupby(['autor','token'])['token'] \
 .count() \
 .reset_index(name='count') \
 .groupby('autor') \
 .apply(lambda x: x.sort_values('count', ascending=False).head(5))

"""En la tabla anterior puede observarse que los términos más frecuentes en todos los usuarios se corresponden con artículos, preposiciones, pronombres…, en general, palabras que no aportan información relevante sobre el texto. Ha estas palabras se les conoce como stopwords. Para cada idioma existen distintos listados de stopwords, además, dependiendo del contexto, puede ser necesario adaptar el listado. Por ejemplo, en la tabla anterior aparece el término amp que procede de la etiqueta html &amp. Con frecuencia, a medida que se realiza un análisis se encuentran palabras que deben incluirse en el listado de stopwords."""

# Obtención de listado de stopwords del inglés
# ==============================================================================
stop_words = list(stopwords.words('spanish'))
print(stop_words[:10])

stop_words.extend(("paraguay","gran","si", "santipenap", "payo","paraguayos","euclidespresidente","rt","mil"))

# Filtrado para excluir stopwords
# ==============================================================================
tweets_tidy = tweets_tidy[~(tweets_tidy["token"].isin(stop_words))]

# Top 10 palabras por autor (sin stopwords)
# ==============================================================================
fig, axs = plt.subplots(nrows=4, ncols=1,figsize=(6, 7))
for i, autor in enumerate(tweets_tidy.autor.unique()):
    df_temp = tweets_tidy[tweets_tidy.autor == autor]
    counts  = df_temp['token'].value_counts(ascending=False).head(10)
    counts.plot(kind='barh', color='firebrick', ax=axs[i])
    axs[i].invert_yaxis()
    axs[i].set_title(autor)

fig.tight_layout()

tweets_tidy[tweets_tidy["autor"] == 'Euclides Acevedo']

"""**Correlación entre autores**

Una forma de cuantificar la similitud entre los perfiles de dos usuarios de Twitter es calculando la correlación en el uso de palabras. La idea es que, si dos usuarios escriben de forma similar, tenderán a utilizar las mismas palabras y con frecuencias similares. La medida de similitud más utilizada al trabajar con texto es 1 - distancia coseno.

Para poder generar los estudios de correlación se necesita disponer de cada variable en una columna. En este caso, las variables a correlacionar son los autores.
"""

# Pivotado de datos
# ==============================================================================
tweets_pivot = tweets_tidy.groupby(["autor","token"])["token"] \
                .agg(["count"]).reset_index() \
                .pivot(index = "token" , columns="autor", values= "count")
tweets_pivot.columns.name = None

# Test de correlación (coseno) por el uso y frecuencia de palabras
# ==============================================================================
from scipy.spatial.distance import cosine

def similitud_coseno(a,b):
    distancia = cosine(a,b)
    return 1-distancia

tweets_pivot.corr(method=similitud_coseno)

tweets_pivot

tweets_pivot.head(3)

# Se seleccionan y renombran las columnas de interés
tweets_pivot = tweets_pivot[['Efraín Alegre', 'Euclides Acevedo', 'Paraguayo Cubas Colomés', 'Santiago Peña']]
tweets_pivot.columns = ['efrain_alegre', 'euclides_acevedo', 'paraguayo_cubas', 'santiago_peña']

# Gráfico de correlación
# ==============================================================================
f, ax = plt.subplots(figsize=(6, 4))
temp = tweets_pivot.dropna()
sns.regplot(
    x  = np.log(temp.efrain_alegre),
    y  = np.log(temp.santiago_peña),
    scatter_kws =  {'alpha': 0.05},
    ax = ax
);
for i in np.random.choice(range(temp.shape[0]), 100):
    ax.annotate(
        text  = temp.index[i],
        xy    = (np.log(temp.efrain_alegre[i]), np.log(temp.santiago_peña[i])),
        alpha = 0.7
    )

# Número de palabras comunes
# ==============================================================================
palabras_efrain = set(tweets_tidy[tweets_tidy.autor == 'Efraín Alegre']['token'])
palabras_santi = set(tweets_tidy[tweets_tidy.autor == 'Santiago Peña']['token'])
palabras_euclides = set(tweets_tidy[tweets_tidy.autor == 'Euclides Acevedo']['token'])
palabras_payo = set(tweets_tidy[tweets_tidy.autor == 'Paraguayo Cubas Colomés']['token'])

print(f"Palabras comunes entre Efrain y Santi: {len(palabras_efrain.intersection(palabras_santi))}")

print(f"Palabras comunes entre Efrain y Euclides: {len(palabras_efrain.intersection(palabras_euclides))}")

print(f"Palabras comunes entre Efrain y Payo: {len(palabras_efrain.intersection(palabras_payo))}")

print(f"Palabras comunes entre Santi y Euclides: {len(palabras_santi.intersection(palabras_euclides))}")

print(f"Palabras comunes entre Santi y Payo: {len(palabras_santi.intersection(palabras_payo))}")

print(f"Palabras comunes entre Euclides y Payo: {len(palabras_euclides.intersection(palabras_payo))}")

"""**Análisis de sentimientos**

Una forma de analizar el sentimiento de un de un texto es considerando su sentimiento como la suma de los sentimientos de cada una de las palabras que lo forman. Esta no es la única forma de abordar el análisis de sentimientos, pero consigue un buen equilibrio entre complejidad y resultados.

Para llevar a cabo esta aproximación es necesario disponer de un diccionario en el que se asocie a cada palabra un sentimiento o nivel de sentimiento. A estos diccionarios también se les conoce como sentiment lexicon. Uno de los más utilizados es:

AFINN: en él, se asigna a cada palabra un valor entre -5 y 5, siendo -5 el máximo de negatividad y +5 el máximo de positividad. Se puede acceder al diccionario a través del repositorio:
https://raw.githubusercontent.com/jboscomendoza/rpubs/master/sentimientos_afinn/lexico_afinn.en.es.csv
"""

# Descarga lexicon sentimientos
# ==============================================================================
lexicon = pd.read_csv(r'https://raw.githubusercontent.com/jboscomendoza/rpubs/master/sentimientos_afinn/lexico_afinn.en.es.csv', engine='python',encoding = "ISO-8859-1")
lexicon.head()

tweets_tidy

# Sentimiento promedio de cada tweet
# ==============================================================================
tweets_sentimientos = pd.merge(
                            left     = tweets_tidy,
                            right    = lexicon,
                            left_on  = "token", 
                            right_on = "Palabra",
                            how      = "inner"
                      )

tweets_sentimientos

tweets_sentimientos = tweets_sentimientos.drop(columns =['Palabra', 'Word'])

tweets_sentimientos

# Se suman los sentimientos de las palabras que forman cada tweet.
tweets_sentimientos_sumatoria = tweets_sentimientos[["autor","fecha", "id", "Puntuacion"]] \
                      .groupby(["autor", "fecha", "id"])\
                      .sum().reset_index()

tweets_sentimientos_sumatoria

"""**Tweets positivos, negativos y neutros**"""

def perfil_sentimientos(df):
    print(autor)
    print("=" * 12)
    print(f"Positivos: {round(100 * np.mean(df.Puntuacion > 0), 2)}")
    ## print(f"Neutros  : {round(100 * np.mean(df.Puntuacion == 0), 2)}")
    print(f"Negativos: {round(100 * np.mean(df.Puntuacion < 0), 2)}")
    print(" ")

for autor, df in tweets_sentimientos.groupby("autor"):
    perfil_sentimientos(df)

"""Tres cantidatos  tienen un perfil muy similar. La gran mayoría de tweets son de tipo positivo. Este patrón es común en redes sociales, donde se suele participar mostrando aspectos o actividades positivas. Los usuarios no tienden a mostrar las cosas malas de sus vidas. El unico candidato que tiene alto porcentaje de tweets tipo negativo es Paraguayo Cubas, quien se identifica como un candidato controversial

**Evolución temporal**


A continuación, veremos cómo varía el sentimiento promedio de los tweets agrupados por intervalos de un mes para cada uno de los usuarios.
"""

fig, ax = plt.subplots(figsize=(7, 4)) 

for autor in tweets_sentimientos.autor.unique():
    df = tweets_sentimientos[tweets_sentimientos.autor == autor].copy()
    df = df.set_index("fecha")
    df = df[['Puntuacion']].resample('1M').mean()
    ax.plot(df.index, df.Puntuacion, label=autor)

ax.set_title("Sentimiento promedio de los tweets por mes")
ax.legend();

"""**Bibliografía**

https://www.cienciadedatos.net/documentos/py25-text-mining-python.html

Text Mining with R: A Tidy Approach libro

Search Engines: Information Retrieval in Practice by Trevor Strohman, Donald Metzler, W. Bruce Croft

http://varianceexplained.org/r/trump-tweets/

http://programminghistorian.github.io/ph-submissions/lessons/published/basic-text-processing-in-r

http://cfss.uchicago.edu/fall2016/text01.html
"""